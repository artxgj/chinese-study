# 混合专家

Search [mdbg](https://www.mdbg.net/chinese/dictionary?page=worddict&wdrst=0&wdqb=混合专家) for definition

Search [wiktionary](https://en.wiktionary.org/wiki/混合专家) for definition

### Tweets containing 混合专家

___
##### 2025-02-25 06:55:41 UTC ~ 外汇交易员
> RT @myfxtrader: DeepSeek“开源周”第二天，介绍DeepEP——首个为混合专家模型（MoE）模型训练和推理设计的开源EP通信库，支持NVLink和RDMA确保高带宽通信；支持FP8降低计算成本；高吞吐量和低延迟内核分别优化训练和推理阶段；重叠机制提升GPU…

[Google Translation](https://translate.google.com/?hi=en&tab=TT&sl=zh-CN&tl=en&op=translate&text=RT+%40myfxtrader%3A+DeepSeek%E2%80%9C%E5%BC%80%E6%BA%90%E5%91%A8%E2%80%9D%E7%AC%AC%E4%BA%8C%E5%A4%A9%EF%BC%8C%E4%BB%8B%E7%BB%8DDeepEP%E2%80%94%E2%80%94%E9%A6%96%E4%B8%AA%E4%B8%BA%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88MoE%EF%BC%89%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E8%AE%BE%E8%AE%A1%E7%9A%84%E5%BC%80%E6%BA%90EP%E9%80%9A%E4%BF%A1%E5%BA%93%EF%BC%8C%E6%94%AF%E6%8C%81NVLink%E5%92%8CRDMA%E7%A1%AE%E4%BF%9D%E9%AB%98%E5%B8%A6%E5%AE%BD%E9%80%9A%E4%BF%A1%EF%BC%9B%E6%94%AF%E6%8C%81FP8%E9%99%8D%E4%BD%8E%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC%EF%BC%9B%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%E5%92%8C%E4%BD%8E%E5%BB%B6%E8%BF%9F%E5%86%85%E6%A0%B8%E5%88%86%E5%88%AB%E4%BC%98%E5%8C%96%E8%AE%AD%E7%BB%83%E5%92%8C%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%EF%BC%9B%E9%87%8D%E5%8F%A0%E6%9C%BA%E5%88%B6%E6%8F%90%E5%8D%87GPU%E2%80%A6)
##### Other Words/Names of Interest in the Above Tweet
[DeepSeek](DeepSeek.md), [优化](优化.md), [吞吐量](吞吐量.md), [型](型.md), [延迟](延迟.md), [开源](开源.md), [成本](成本.md), [推理](推理.md), [支持](支持.md), [模型](模型.md), [确保](确保.md), [设计](设计.md), [重叠](重叠.md), [阶段](阶段.md), [降低](降低.md)
___
##### 2025-02-24 16:17:04 UTC ~ GitHubDaily
> RT @GitHub_Daily: 分享一份《从零实现稀疏 MoE (混合专家) 模型》教程。本教材深入讲解 MoE 模型核心组件，包括自注意力机制、专家网络和 Top-k 路由 等。教程: https://t.co/9kmUg9PLNU并通过代码展示注意力层、专家…

[Google Translation](https://translate.google.com/?hi=en&tab=TT&sl=zh-CN&tl=en&op=translate&text=RT+%40GitHub_Daily%3A+%E5%88%86%E4%BA%AB%E4%B8%80%E4%BB%BD%E3%80%8A%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E7%A8%80%E7%96%8F+MoE+%28%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%29+%E6%A8%A1%E5%9E%8B%E3%80%8B%E6%95%99%E7%A8%8B%E3%80%82%E6%9C%AC%E6%95%99%E6%9D%90%E6%B7%B1%E5%85%A5%E8%AE%B2%E8%A7%A3+MoE+%E6%A8%A1%E5%9E%8B%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%EF%BC%8C%E5%8C%85%E6%8B%AC%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E3%80%81%E4%B8%93%E5%AE%B6%E7%BD%91%E7%BB%9C%E5%92%8C+Top-k+%E8%B7%AF%E7%94%B1+%E7%AD%89%E3%80%82%E6%95%99%E7%A8%8B%3A+https%3A%2F%2Ft.co%2F9kmUg9PLNU%E5%B9%B6%E9%80%9A%E8%BF%87%E4%BB%A3%E7%A0%81%E5%B1%95%E7%A4%BA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E3%80%81%E4%B8%93%E5%AE%B6%E2%80%A6)
##### Other Words/Names of Interest in the Above Tweet
[包括](包括.md), [型](型.md), [实现](实现.md), [核心](核心.md), [模型](模型.md)
___
##### 2025-01-04 08:13:12 UTC ~ 雁过留声
> RT @szygls: 中国deepseek是一款大型语言模型，被称为DeepSeek-V3，具有6710亿参数，采用了混合专家（MoE）架构。每次推理仅激活370亿参数，显著降低了计算开销。在14.8万亿高质量token上进行了预训练，涵盖了数学、编程、中文等多个领域。… h…

[Google Translation](https://translate.google.com/?hi=en&tab=TT&sl=zh-CN&tl=en&op=translate&text=RT+%40szygls%3A+%E4%B8%AD%E5%9B%BDdeepseek%E6%98%AF%E4%B8%80%E6%AC%BE%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%A2%AB%E7%A7%B0%E4%B8%BADeepSeek-V3%EF%BC%8C%E5%85%B7%E6%9C%896710%E4%BA%BF%E5%8F%82%E6%95%B0%EF%BC%8C%E9%87%87%E7%94%A8%E4%BA%86%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%EF%BC%88MoE%EF%BC%89%E6%9E%B6%E6%9E%84%E3%80%82%E6%AF%8F%E6%AC%A1%E6%8E%A8%E7%90%86%E4%BB%85%E6%BF%80%E6%B4%BB370%E4%BA%BF%E5%8F%82%E6%95%B0%EF%BC%8C%E6%98%BE%E8%91%97%E9%99%8D%E4%BD%8E%E4%BA%86%E8%AE%A1%E7%AE%97%E5%BC%80%E9%94%80%E3%80%82%E5%9C%A814.8%E4%B8%87%E4%BA%BF%E9%AB%98%E8%B4%A8%E9%87%8Ftoken%E4%B8%8A%E8%BF%9B%E8%A1%8C%E4%BA%86%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E6%B6%B5%E7%9B%96%E4%BA%86%E6%95%B0%E5%AD%A6%E3%80%81%E7%BC%96%E7%A8%8B%E3%80%81%E4%B8%AD%E6%96%87%E7%AD%89%E5%A4%9A%E4%B8%AA%E9%A2%86%E5%9F%9F%E3%80%82%E2%80%A6+h%E2%80%A6)
##### Other Words/Names of Interest in the Above Tweet
[DeepSeek](DeepSeek.md), [型](型.md), [大型语言模型](大型语言模型.md), [推理](推理.md), [模型](模型.md), [涵盖](涵盖.md), [进行](进行.md), [降低](降低.md), [领域](领域.md)
